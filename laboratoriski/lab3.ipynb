{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T17:42:46.698505Z",
     "start_time": "2025-12-09T17:42:36.776314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from evaluate import load\n",
    "from seq2seq import create_transformers_train_data, train_transformer, decode_with_transformer"
   ],
   "id": "49106b873b7fdc50",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsan\\PycharmProjects\\opj\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T17:42:46.730022Z",
     "start_time": "2025-12-09T17:42:46.705507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "b60a31a27a3d9c49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T17:42:46.760024Z",
     "start_time": "2025-12-09T17:42:46.746024Z"
    }
   },
   "cell_type": "code",
   "source": "data = pd.read_csv('../yelp_parallel/yelp_parallel/test_en_parallel.txt', sep='\\t')",
   "id": "78e139f898030f24",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T17:42:46.792022Z",
     "start_time": "2025-12-09T17:42:46.777023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "negative = data[\"Style 1\"].values.tolist()\n",
    "positive = data[\"Style 2\"].values.tolist()"
   ],
   "id": "98dd062a82122405",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T17:42:49.737382Z",
     "start_time": "2025-12-09T17:42:46.809028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bleu = load(\"bleu\")\n",
    "bertscore = load(\"bertscore\")"
   ],
   "id": "7fb9fa6fdd29bbdf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T18:01:47.118464Z",
     "start_time": "2025-12-09T18:01:47.104465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(model_name, negative, positive, lr=0.001, epochs=5, num_examples=30, batch_size=256, device=None):\n",
    "    print(f\"Model: {model_name}, Learning rate: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "    train_dataset = create_transformers_train_data(negative, positive, tokenizer)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    train_transformer(model, train_loader, optimizer, epochs, device=device)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    all_inputs = []\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        input_text = negative[i]\n",
    "        reference = positive[i]\n",
    "\n",
    "        prediction = decode_with_transformer(input_text, tokenizer, model)\n",
    "\n",
    "        all_inputs.append(input_text)\n",
    "        all_predictions.append(prediction)\n",
    "        all_references.append(reference)\n",
    "\n",
    "        # print(\"Negative: \", input_text)\n",
    "        # print(\"Prediction: \", prediction)\n",
    "        # print(\"Positive: \", reference)\n",
    "\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=all_predictions,\n",
    "        references=all_references\n",
    "    )\n",
    "\n",
    "    bert_score = bertscore.compute(\n",
    "        predictions=all_predictions,\n",
    "        references=all_references,\n",
    "        lang=\"en\" #model_type='microsoft/deberta-xlarge-mnli' predolgo trae koga se koristi ovoj model_type pa zatoa koristam lang='en' bidejki samiot bert_score barashe barem edno od ovie da bide navedeno\n",
    "    )\n",
    "\n",
    "    avg_bert_f1 = sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"])\n",
    "\n",
    "    print(\"BLEU:\", bleu_score)\n",
    "    print(\"BERTScore f1: \", avg_bert_f1)\n",
    "    # print(\"BERTScore: \", bert_score) premnogu golem output dava pa go skrativ za da bide popregledno\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"BERTScore average f1\": avg_bert_f1,\n",
    "        # \"bertscore\": bert_score,\n",
    "    }"
   ],
   "id": "532351a37c5429c6",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T18:01:55.995247Z",
     "start_time": "2025-12-09T18:01:47.699847Z"
    }
   },
   "cell_type": "code",
   "source": "test = run_experiment(\"t5-small\", negative, positive, lr=0.001, epochs=3,num_examples=30,device=device)",
   "id": "16e10d3e8f37a067",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-small, Learning rate: 0.001, Epochs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsan\\PycharmProjects\\opj\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 2.9925\n",
      "Epoch 2/3, Loss: 2.3542\n",
      "Epoch 3/3, Loss: 2.1506\n",
      "BLEU: {'bleu': 0.2867368699753908, 'precisions': [0.5934579439252337, 0.41304347826086957, 0.3051948051948052, 0.22580645161290322], 'brevity_penalty': 0.7953508327485446, 'length_ratio': 0.8136882129277566, 'translation_length': 214, 'reference_length': 263}\n",
      "BERTScore f1:  0.9140923142433166\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T18:01:56.041469Z",
     "start_time": "2025-12-09T18:01:56.027469Z"
    }
   },
   "cell_type": "code",
   "source": "results = []",
   "id": "7f3c79448171121f",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T18:04:00.812395Z",
     "start_time": "2025-12-09T18:01:59.207154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results.append(run_experiment(\"t5-small\", negative, positive, lr=0.001, epochs=3,num_examples=300,device=device))\n",
    "results.append(run_experiment(\"t5-small\", negative, positive, lr=0.001, epochs=5,num_examples=300,device=device))\n",
    "results.append(run_experiment(\"t5-small\", negative, positive, lr=0.0001, epochs=10,num_examples=300,device=device))"
   ],
   "id": "f06c54d808303a87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: t5-small, Learning rate: 0.001, Epochs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsan\\PycharmProjects\\opj\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 2.9939\n",
      "Epoch 2/3, Loss: 2.3969\n",
      "Epoch 3/3, Loss: 2.1875\n",
      "BLEU: {'bleu': 0.25250003970319285, 'precisions': [0.5709555345316935, 0.4013230429988975, 0.29260237780713344, 0.21499176276771004], 'brevity_penalty': 0.7287229327644853, 'length_ratio': 0.7596119295724039, 'translation_length': 2114, 'reference_length': 2783}\n",
      "BERTScore f1:  0.9128046178817749\n",
      "Model: t5-small, Learning rate: 0.001, Epochs: 5\n",
      "Epoch 1/5, Loss: 3.0066\n",
      "Epoch 2/5, Loss: 2.3683\n",
      "Epoch 3/5, Loss: 2.1621\n",
      "Epoch 4/5, Loss: 2.0051\n",
      "Epoch 5/5, Loss: 1.8727\n",
      "BLEU: {'bleu': 0.2594070106196202, 'precisions': [0.5810684161199625, 0.4116684841875682, 0.2953063885267275, 0.21636952998379255], 'brevity_penalty': 0.7377695903399782, 'length_ratio': 0.766798418972332, 'translation_length': 2134, 'reference_length': 2783}\n",
      "BERTScore f1:  0.9149351690212886\n",
      "Model: t5-small, Learning rate: 0.0001, Epochs: 10\n",
      "Epoch 1/10, Loss: 3.5105\n",
      "Epoch 2/10, Loss: 3.1570\n",
      "Epoch 3/10, Loss: 2.9386\n",
      "Epoch 4/10, Loss: 2.8073\n",
      "Epoch 5/10, Loss: 2.6978\n",
      "Epoch 6/10, Loss: 2.6191\n",
      "Epoch 7/10, Loss: 2.5551\n",
      "Epoch 8/10, Loss: 2.4829\n",
      "Epoch 9/10, Loss: 2.4272\n",
      "Epoch 10/10, Loss: 2.4013\n",
      "BLEU: {'bleu': 0.2281950452628886, 'precisions': [0.5460557392536608, 0.3670886075949367, 0.25774555042847724, 0.18472906403940886], 'brevity_penalty': 0.7300836761145013, 'length_ratio': 0.7606899029823931, 'translation_length': 2117, 'reference_length': 2783}\n",
      "BERTScore f1:  0.9090086128314336\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T18:06:20.682819Z",
     "start_time": "2025-12-09T18:04:00.853443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results.append(run_experiment(\"google/flan-t5-small\", negative, positive, lr=0.001, epochs=3,num_examples=300,device=device))\n",
    "results.append(run_experiment(\"google/flan-t5-small\", negative, positive, lr=0.001, epochs=5,num_examples=300,device=device))\n",
    "results.append(run_experiment(\"google/flan-t5-small\", negative, positive, lr=0.0001, epochs=10,num_examples=300,device=device))"
   ],
   "id": "15f6765279e845a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/flan-t5-small, Learning rate: 0.001, Epochs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsan\\PycharmProjects\\opj\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 2.5855\n",
      "Epoch 2/3, Loss: 2.0798\n",
      "Epoch 3/3, Loss: 1.8712\n",
      "BLEU: {'bleu': 0.2599353402769382, 'precisions': [0.5741000467508182, 0.4067427949972811, 0.2969460688758934, 0.2195318805488297], 'brevity_penalty': 0.7400220700644776, 'length_ratio': 0.768595041322314, 'translation_length': 2139, 'reference_length': 2783}\n",
      "BERTScore f1:  0.9138826098044713\n",
      "Model: google/flan-t5-small, Learning rate: 0.001, Epochs: 5\n",
      "Epoch 1/5, Loss: 2.5633\n",
      "Epoch 2/5, Loss: 2.0781\n",
      "Epoch 3/5, Loss: 1.8683\n",
      "Epoch 4/5, Loss: 1.7021\n",
      "Epoch 5/5, Loss: 1.5397\n",
      "BLEU: {'bleu': 0.279262527507512, 'precisions': [0.5868971792538672, 0.42571127502634354, 0.311639549436796, 0.22650231124807396], 'brevity_penalty': 0.766323368368248, 'length_ratio': 0.789795185052102, 'translation_length': 2198, 'reference_length': 2783}\n",
      "BERTScore f1:  0.9174800852934519\n",
      "Model: google/flan-t5-small, Learning rate: 0.0001, Epochs: 10\n",
      "Epoch 1/10, Loss: 2.9174\n",
      "Epoch 2/10, Loss: 2.6556\n",
      "Epoch 3/10, Loss: 2.4727\n",
      "Epoch 4/10, Loss: 2.3624\n",
      "Epoch 5/10, Loss: 2.2890\n",
      "Epoch 6/10, Loss: 2.2170\n",
      "Epoch 7/10, Loss: 2.1572\n",
      "Epoch 8/10, Loss: 2.1210\n",
      "Epoch 9/10, Loss: 2.0795\n",
      "Epoch 10/10, Loss: 2.0563\n",
      "BLEU: {'bleu': 0.26393753120946123, 'precisions': [0.5550089445438283, 0.3899793388429752, 0.28361858190709044, 0.21032934131736528], 'brevity_penalty': 0.7829916292174781, 'length_ratio': 0.8034495149119655, 'translation_length': 2236, 'reference_length': 2783}\n",
      "BERTScore f1:  0.9146471828222275\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T18:06:20.713330Z",
     "start_time": "2025-12-09T18:06:20.698823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(results)):\n",
    "    print(f\"{i}: {results[i]}\")"
   ],
   "id": "1439fe2c74a7b38f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {'bleu': {'bleu': 0.25250003970319285, 'precisions': [0.5709555345316935, 0.4013230429988975, 0.29260237780713344, 0.21499176276771004], 'brevity_penalty': 0.7287229327644853, 'length_ratio': 0.7596119295724039, 'translation_length': 2114, 'reference_length': 2783}, 'BERTScore average f1': 0.9128046178817749}\n",
      "1: {'bleu': {'bleu': 0.2594070106196202, 'precisions': [0.5810684161199625, 0.4116684841875682, 0.2953063885267275, 0.21636952998379255], 'brevity_penalty': 0.7377695903399782, 'length_ratio': 0.766798418972332, 'translation_length': 2134, 'reference_length': 2783}, 'BERTScore average f1': 0.9149351690212886}\n",
      "2: {'bleu': {'bleu': 0.2281950452628886, 'precisions': [0.5460557392536608, 0.3670886075949367, 0.25774555042847724, 0.18472906403940886], 'brevity_penalty': 0.7300836761145013, 'length_ratio': 0.7606899029823931, 'translation_length': 2117, 'reference_length': 2783}, 'BERTScore average f1': 0.9090086128314336}\n",
      "3: {'bleu': {'bleu': 0.2599353402769382, 'precisions': [0.5741000467508182, 0.4067427949972811, 0.2969460688758934, 0.2195318805488297], 'brevity_penalty': 0.7400220700644776, 'length_ratio': 0.768595041322314, 'translation_length': 2139, 'reference_length': 2783}, 'BERTScore average f1': 0.9138826098044713}\n",
      "4: {'bleu': {'bleu': 0.279262527507512, 'precisions': [0.5868971792538672, 0.42571127502634354, 0.311639549436796, 0.22650231124807396], 'brevity_penalty': 0.766323368368248, 'length_ratio': 0.789795185052102, 'translation_length': 2198, 'reference_length': 2783}, 'BERTScore average f1': 0.9174800852934519}\n",
      "5: {'bleu': {'bleu': 0.26393753120946123, 'precisions': [0.5550089445438283, 0.3899793388429752, 0.28361858190709044, 0.21032934131736528], 'brevity_penalty': 0.7829916292174781, 'length_ratio': 0.8034495149119655, 'translation_length': 2236, 'reference_length': 2783}, 'BERTScore average f1': 0.9146471828222275}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "instruction_prompt = \"Translate this negative review into a positive one: \"",
   "id": "7518828d23eb4f87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "instructional_negative = [instruction_prompt + sentence for sentence in negative]",
   "id": "5c2d9e912abf8ce1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
